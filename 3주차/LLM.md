# 1장. 대규모 언어 모델 소개

## 1.1 언어 AI란?

* **정의**: 인간 언어를 이해·처리·생성할 수 있는 AI 기술.
* **NLP와 차이점**: NLP와 거의 동일하게 쓰이지만, LLM 전반을 포함해 더 넓게 사용.
* **적용 예시**: 번역, 챗봇, 요약, 검색, 질의응답 등.

---

## 1.2 언어 AI의 최근 역사

### BoW (Bag of Words)

* **방식**: 문장을 단어로 나누고, 단어 출현 횟수를 벡터로 표현.
* **장점**: 구현 간단.
* **단점**: 문맥·의미 반영 불가.

### 밀집 벡터 임베딩 (Dense Vector Embedding)

* **word2vec** (2013)

  * 단어 의미를 수치 벡터로 학습.
  * 비슷한 의미의 단어는 벡터 공간에서 가깝게 위치.
* **장점**: 의미 유사성 반영 가능.

### 문맥 기반 임베딩

* **한계**: word2vec은 문맥 변화 반영 불가.
* **해결책**: RNN + 어텐션(Attention) → 문맥별 표현 가능.

---

## 1.3 어텐션과 트랜스포머

* **어텐션(Attention)**: 입력 시퀀스에서 중요한 부분에 가중치를 높게 주는 메커니즘.
* **트랜스포머 (2017)**:

  * RNN 제거, 어텐션만 사용.
  * 병렬 학습 가능 → 훈련 속도 향상.
  * 인코더·디코더 구조로 다양한 NLP 태스크 수행.

---

## 1.4 표현 모델 vs 생성 모델

* **표현 모델 (인코더 기반)**:

  * 예: BERT
  * 목적: 분류, 검색 등 입력 분석.
* **생성 모델 (디코더 기반)**:

  * 예: GPT
  * 목적: 텍스트 생성.

---

## 1.5 대규모 언어 모델의 정의와 특성

* **정의**: 파라미터 수가 매우 많고, 대규모 데이터로 학습된 언어 모델.
* **‘대규모’의 의미**: 절대적인 기준은 없음.
* **특징**:

  * 광범위한 태스크 처리 가능.
  * 사전 학습 후 다양한 방식으로 활용 가능.

---

## 1.6 LLM의 훈련 패러다임

* **지도 학습**: 정답이 있는 데이터로 학습.
* **비지도 학습**: 정답 없는 데이터에서 패턴 학습.
* **자기지도 학습**: 일부 데이터를 가리고 예측하도록 학습.

---

## 1.7 LLM의 주요 활용 사례

* 번역
* 텍스트 생성
* 요약
* 질문 응답
* 시맨틱 검색

---

## 1.8 LLM 인터페이스

* **독점 모델**: OpenAI GPT, Anthropic Claude 등.
* **오픈 모델**: LLaMA, Mistral 등.
* **오픈소스 프레임워크**: Hugging Face Transformers 등.

---

## 1.9 첫 번째 텍스트 생성 예제

* API 키 발급 → LLM 호출 → 프롬프트 입력 → 결과 확인.

---

## 1.10 요약

* LLM은 언어 AI 발전의 핵심.
* BoW → 임베딩 → RNN+어텐션 → 트랜스포머로 발전.
* 생성·표현 모델 모두 다양한 분야에서 활용 가능.
* GPU·클라우드 환경에서 쉽게 실습 가능.

---
