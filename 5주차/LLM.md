# 📘 3장 대규모 언어 모델 자세히 살펴보기

## 3.1 트랜스포머 모델 개요

### 3.1.1 훈련된 트랜스포머 LLM의 입력과 출력

* 입력 텍스트는 **토큰화 → 임베딩 → 모델 입력** 과정을 거침.
* 출력은 각 토큰에 대해 **다음 단어 등장 확률 분포**를 예측.
* 이를 통해 문장 생성, 번역, 요약 같은 다양한 작업 수행.

### 3.1.2 정방향 연산

* 입력 토큰이 순차적으로 **임베딩 → 트랜스포머 블록 → 출력층**을 통과.
* 모델은 **다층 구조**를 통해 문맥적 의미를 학습.

### 3.1.3 샘플링과 디코딩

* 출력 확률 분포에서 **다음 토큰을 선택하는 방식**이 다양함:

  * **Greedy**: 가장 확률 높은 토큰 선택.
  * **Sampling**: 확률 분포에서 무작위 선택.
  * **Top-k, Top-p (Nucleus)**: 일정 범위 내 토큰만 후보로 선택.

### 3.1.4 병렬 토큰 처리와 문맥 크기

* 트랜스포머는 RNN과 달리 **병렬 연산**이 가능.
* 다만 **문맥 창(window)** 크기에 따라 모델 성능과 한계가 결정됨.

### 3.1.5 캐싱

* 생성 과정에서 이미 계산된 **Key, Value**를 캐싱하여 속도 향상.
* 긴 텍스트 생성 시 효율성을 크게 높임.

### 3.1.6 트랜스포머 블록 내부

* 각 블록은 크게 **셀프 어텐션(Self-Attention)** 과 **피드포워드 네트워크**로 구성.
* 셀프 어텐션:

  * 각 토큰이 다른 토큰과의 **관련성(Attention Score)** 을 계산.
  * **Softmax 정규화**로 가중치를 적용, 값(Value) 벡터와 곱해 정보 통합.
* 잔차 연결(residual)과 정규화(layer norm)를 통해 안정적 학습 지원.

---

## 3.2 트랜스포머 아키텍처의 최근 발전

### 3.2.1 효율적인 어텐션

* 어텐션 연산은 계산량이 크기 때문에 다양한 최적화 연구 진행.
* **로컬/희소 어텐션**: 전체 토큰 대신 일부 범위(윈도우)에만 집중.
* **슬라이딩 윈도우**: 긴 문서를 다룰 때 유용.

### 3.2.2 트랜스포머 블록 변형

* 블록 구조 자체에 변화를 줘 성능 향상 시도.
* 예: **RMSNorm, SwiGLU 활성화 함수** 등.

### 3.2.3 위치 임베딩 (RoPE)

* 순서 정보를 반영하기 위해 위치 임베딩 사용.
* RoPE(Rotary Position Embedding)는 각 토큰의 위치 정보를 **회전 변환 방식**으로 부여, 긴 문맥 처리에 강점.

### 3.2.4 실험적 구조 및 개선 사항

* **FlashAttention**: 어텐션 계산을 GPU 친화적으로 최적화해 메모리 사용량 절감.
* 다양한 변형 모델(GPT, LLaMA 등)에서 이 기법들이 적용됨.

---

## 3.3 요약

* 트랜스포머는 **셀프 어텐션**을 핵심으로 하는 구조.
* 입력 토큰을 병렬로 처리하여 RNN보다 효율적이며 긴 문맥을 학습 가능.
* 최근 연구들은 **효율성(희소 어텐션, 캐싱, FlashAttention)** 과 **표현력(RoPE, SwiGLU, 새로운 정규화 방식)** 강화를 중심으로 발전 중.
* 이 장은 **LLM이 실제로 텍스트를 이해·생성하는 내부 메커니즘**을 이해하는 기반을 제공.

---