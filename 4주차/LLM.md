## 📘 2장 요약: 토큰과 임베딩

---

### 1. 토큰화 (Tokenization)

* **정의**: 텍스트를 모델이 이해할 수 있는 작은 단위(토큰)로 나누는 과정.
* **토큰의 종류**

  * **단어 토큰(word token)**: 공백 단위 분할.
  * **부분 단어 토큰(subword token)**: BPE(Byte Pair Encoding), SentencePiece 같은 알고리즘 기반.
  * **문자 토큰(character token)**: 각 문자를 분리.
  * **바이트 토큰(byte token)**: 멀티바이트 문자까지 처리 가능.
* **LLM 동작**: 모델은 입력을 토큰 단위로 받아들이고 출력도 한 번에 하나씩 토큰을 생성함.
* **토크나이저 비교**: GPT, BERT, LLaMA 등 모델별 토크나이저가 다르며, 코드/다국어 처리에서 특성이 달라짐.
* **속성**: 어휘 크기, 희귀 단어 처리 방식, 다국어 지원 능력 등이 모델 성능과 직결됨.

---

### 2. 토큰 임베딩 (Token Embeddings)

* **정의**: 토큰을 의미를 담은 수치 벡터로 변환하는 과정.
* **역할**: 모델이 단어 간의 의미적 유사성과 문맥적 패턴을 학습할 수 있도록 지원.
* **임베딩 방식**:

  * 토크나이저 어휘사전에 매핑된 기본 임베딩.
  * 문맥을 반영한 동적 임베딩(예: 트랜스포머 기반).
* **효과**: 문맥에 맞는 단어 의미를 반영하여 “bank(은행/강둑)”처럼 다의어 문제 해결.

---

### 3. 문장 및 문서 임베딩

* **문장 임베딩**: 개별 단어가 아닌 전체 문장의 의미를 하나의 벡터로 표현.
* **문서 임베딩**: 대규모 문서나 단락 수준의 의미 표현.
* **활용**: 시맨틱 검색, 텍스트 분류, 토픽 모델링 등에서 핵심적으로 사용됨.

---

### 4. 전통적 임베딩 기법

* **word2vec**

  * 주변 단어 예측을 통해 단어 의미를 학습하는 초기 임베딩 기법.
  * 비슷한 의미의 단어들이 벡터 공간에서 가까이 위치.
* **대조 학습(contrastive learning)**: 서로 다른 문맥에서 단어의 관계를 학습.

---

### 5. 추천 시스템 응용

* **음악 추천 사례**: 노래 가사/메타데이터를 임베딩으로 변환하여 유사도 기반 추천.
* **임베딩 모델 훈련**: 사용자 취향 반영 가능, 시맨틱 기반 필터링에 활용.

---

### ✅ 핵심 요약

* **토큰화**는 “텍스트를 쪼개는 과정”, **임베딩**은 “쪼갠 토큰을 의미 벡터로 변환하는 과정”.
* 토큰과 임베딩을 이해해야 LLM의 입력·출력 방식과 내부 동작을 올바르게 이해할 수 있음.
* word2vec 같은 고전 기법부터 최신 트랜스포머 기반 임베딩까지 발전 과정을 다룸.
* 단어 → 문장 → 문서 임베딩으로 확장되며, **검색, 분류, 추천 시스템** 등 다양한 응용으로 이어짐.

---